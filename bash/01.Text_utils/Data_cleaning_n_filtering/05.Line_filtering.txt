
https://askubuntu.com/questions/1206110/sed-extract-lines-from-text
--------------------------------------------------------------------------------------
##Extract particular lines
awk 'NR==1 || NR==5 || NR==1010' "file"

sed -n -e 1p -e 3p -e  5p

--generate a range
$ for i in {1..100..2}; do echo -n " -e ${i}p"; done
 -e 1p -e 3p -e 5p -e 7p -e 9p -e 11p -e 13p -e 15p -e 17p -e 19p -e 21p -e 23p -e 25p -e 27p

##Extracted odd/even lines
--Get even lines
awk 'NR%2==0' filename
perl -lne 'print if $. % 2 == 0' infile > outfile

--Get odd lines
awk 'NR%2==1' filename
awk '!(NR%2)' file

--Keep odd lines
sed 'n; d' infile > outfile

--Keep even lines
sed '1d; n; d' infile > outfile

 sed -n 'p;n' file # keep odd
 sed -n 'n;p' file # keep even

-n: suppress printing
-p: print current line
-n: next line

--------------------------------------------------------------------------------------
#Extracting lines between patterns
cat output.txt | sed -n '/10:00/, /10:30/p'

-Extract range use comma “,” to separate the range
$awk ‘/Joel/, /Marlene/ {print $3}’ testFile.txt

-print section of file between two regular expressions (inclusive)
awk '/Iowa/,/Montana/'        

-Extract a specific range of lines from a file
$ awk 'NR>=10 && NR<=20 {print}' file.txt

#processing on match
 awk '/regex/{print x};{x=$0}'
--------------------------------------------------------------------------------------
## Line Range extraction
sed -n '100,200p;200q' filename > newfile
head -16482 in.sql | tail -258 > out.sql
awk 'NR>=16224&&NR<=16482' in.sql > out.sql
vim: :16224,16482w!/tmp/some-file
perl -ne 'print if 16224..16482' file.txt > new_file.txt

If the file is huge, it can be good to exit after reading the last desired line. This way, it won't read the following lines unnecessarily:
awk 'NR==16224, NR==16482-1; NR==16482 {print; exit}' file

--Extract range plus extra lines
sed -n -e 1p -e 101,200p file.csv > newfile.csv

--Extract range plus fields
sed -n '10,50p' file.csv | awk -F ';' '{print $3 $4}'

--------------------------------------------------------------------------------------
## Add Header & Trailer
awk -v head="$Bus_date" 'BEGIN { print head } { print } END { print "EOF" NR-1 }'
awk 'BEGIN{print "START"}; {print}; END{print "END"}'
awk 'BEGIN {print "Header Text"} {print $0} END {print "Footer Text"}' test.txt
awk -v header="Header: $(date)" -v footer="Footer: $(hostname)" 'BEGIN {print header} {print $0} END {print footer}' test.txt

start reading from the given file, by the BEGIN block.
The { print } block ensures that the given file is passed through as is, without modification.
The END block uses the built-in variable NR which holds the number of records (lines) read so far. In the END block, NR will be the number of lines in the file that was read.

--Add after each line
awk '{print "START";print;print "END"}'

--Add with sed
sed -e '1s/.*/START\n&/g' -e '$s/.*/&\nEND/g' filename
sed -e '1i Header Text' -e '$a Footer Text' test.txt
sed -i '1i Header Text' DataFile.dat

--Inplace header
sed -i '1i red|blue|green|orange|yellow' file

--dynamic header & footers
sed -e "1i Header: $(date)" -e "$ a Footer: $(hostname)" test.txt

--------------------------------------------------------------------------------------
## split file
split -l 1000 file


## Extract all keys from file
grep -aFf key_file masterfile
grep -Ff ids.txt urls.log
grep -F -f keys textfile

https://stackoverflow.com/questions/19380925/grep-a-large-list-against-a-large-file/42711996#42711996


## Include & exclude patterns
grep -Ff ids.txt urls.log | grep -v /$ > result.txt


## validate field length
awk -F'#' 'NR>1{split($3, a, " ")} length(a[1])>6{print $3}' file


--------------------------------------------------------------------------------------
##Extract values from config file
git config --file /etc/applications.cfg --get APPN.stopScript
git config --file /etc/applications.cfg --list

--------------------------------------------------------------------------------------



--------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------


