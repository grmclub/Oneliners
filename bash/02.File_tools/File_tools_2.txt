--------------------------------------------------------------------------------------
https://www.baeldung.com/linux/csv-parsing

##Reading Records From a File
while read line
do
   echo "Record is : $line"
done < input.csv

##Ignoring the Header Line


#!/bin/bash
while read line
do
   echo "Record is : $line"
done < <(tail -n +2 input.csv)
--------------------------------------------------------------------------------------
##Parse csv columns

#! /bin/bash
while IFS="," read -r rec_column1 rec_column2 rec_column3 rec_column4
do
  echo "Displaying Record-$rec_column1"
  echo "Quantity: $rec_column2"
  echo "Price: $rec_column3"
  echo "Value: $rec_column4"
  echo ""
done < <(tail -n +2 input.csv)

---------------------
Ex2

INPUT=data.csv
OLDIFS=$IFS
IFS=','
[ ! -f $INPUT ] && { echo "$INPUT file not found"; exit 99; }
while read flname dob ssn tel status
do
	echo "Name : $flname"
	echo "DOB : $dob"
	echo "SSN : $ssn"
	echo "Telephone : $tel"
	echo "Status : $status"
done < $INPUT
IFS=$OLDIFS
---------------------



##Mapping Columns of CSV File into Bash Arrays
#! /bin/bash
arr_record1=( $(tail -n +2 input.csv | cut -d ',' -f1) )
arr_record2=( $(tail -n +2 input.csv | cut -d ',' -f2) )
arr_record3=( $(tail -n +2 input.csv | cut -d ',' -f3) )
arr_record4=( $(tail -n +2 input.csv | cut -d ',' -f4) )
echo "array of SNos  : ${arr_record1[@]}"
echo "array of Qty   : ${arr_record2[@]}"
echo "array of Price : ${arr_record3[@]}"
echo "array of Value : ${arr_record4[@]}"
--------------------------------------------------------------------------------------
*csv to xls
ssconvert is a command line utility for Gnumeric, a Linux spreadsheet program. We need to install Gnumeric to get ssconvert:

$ sudo apt install gnumeric
Conversion from CSV to XLS or XLSX is straightforward:

$ ssconvert people-100.csv people-100.xls
$ ssconvert people-100.csv people-100.xlsx

Alternatively, we can do the conversion to XLSX this way:

$ ssconvert --export-type=Gnumeric_Excel:xlsx people-100.csv

--------------------------------------------------------------------------------------
* sort-large-files-efficiently 

##By specifying â€“parallel=4, we instruct the sort command to divide the input data into multiple chunks and sort them in parallel using four threads.
#sort --parallel=4 data.txt > sorted_parallel_data.txt

##Use split tool to break data.txt into smaller files:
$ split -l 10000 -d data.txt splitdata

##sort the splitdata files:
$ for X in splidata*; do sort -t'|' -k2 -n < $X > final-$X; done

##merge the sorted files into one large file:
$ time sort -t'|' -k2 -n -m final-splitdata* > sorted_split_data.txt

----------
###When comparing both performance stats, we see that using the C locale drastically enhances performance.
$ time LC_ALL=C sort data.txt > sorted_data_c_locale.txt
real 0m3.178s
user 0m4.615s
sys 0m0.080s


--------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------
