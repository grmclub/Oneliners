

--------------------------------------------------------------------------------------
* sort-large-files-efficiently 

##By specifying â€“parallel=4, we instruct the sort command to divide the input data into multiple chunks and sort them in parallel using four threads.
#sort --parallel=4 data.txt > sorted_parallel_data.txt

##Use split tool to break data.txt into smaller files:
$ split -l 10000 -d data.txt splitdata

##sort the splitdata files:
$ for X in splidata*; do sort -t'|' -k2 -n < $X > final-$X; done

##merge the sorted files into one large file:
$ time sort -t'|' -k2 -n -m final-splitdata* > sorted_split_data.txt

----------
###When comparing both performance stats, we see that using the C locale drastically enhances performance.
$ time LC_ALL=C sort data.txt > sorted_data_c_locale.txt
real 0m3.178s
user 0m4.615s
sys 0m0.080s


--------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------
